# -*- coding: utf-8 -*-
"""DengAiV2Witt.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TJu9Fwou_qhZBV2iJhM7Vbh819PRFRsr
"""

#I've managed to go from 26.4 to 35, the things that I thought would improve the model are
#Not improving the model.  Oh no!  The things to make for next time are:
  #Bayesian lookback, not sure which implemetation would help, shift is easy
  #Better model, better paramgrid
  #Dropping more columns?
  #Normalizing input variables??

import pandas as pd
import numpy as np


test = pd.read_csv('https://raw.githubusercontent.com/RowenWitt/DengAi/main/DengData/DengAI_Test_Features.csv')
submission = pd.read_csv('https://raw.githubusercontent.com/RowenWitt/DengAi/main/DengData/DengAI_Submission_Format.csv')
train_X = pd.read_csv('https://raw.githubusercontent.com/RowenWitt/DengAi/main/DengData/DengAI_Training_Data_Features.csv')
train_y = pd.read_csv('https://raw.githubusercontent.com/RowenWitt/DengAi/main/DengData/DengAI_Training_Data_Labels.csv')

#All I need to do is split up the cities, train on train_X & train_y, then predict, then put the dfs back together
#IF I need to improve after a successful first submission, I'll just do a cv gridsearch (fuck a time series)

droppers = ['reanalysis_sat_precip_amt_mm','reanalysis_min_air_temp_k',
            'ndvi_nw','precipitation_amt_mm','reanalysis_relative_humidity_percent','station_min_temp_c']

def wrangle(X):
  sj = X.loc[X['city']=='sj']
  sj.drop(columns='city',inplace=True)
  iq = X.loc[X['city']=='iq']
  iq.drop(columns='city',inplace=True)
  return(sj,iq)

X_train_sj,X_train_iq = wrangle(train_X)
y_train_sj,y_train_iq = wrangle(train_y)

test_sj,test_iq = wrangle(test)

def wrangle2(X):
  """Only for features"""
  X.drop(columns=droppers,inplace=True)
  return X

X_train_sj = wrangle2(X_train_sj)
X_train_iq = wrangle2(X_train_iq)

test_sj = wrangle2(test_sj)
test_iq = wrangle2(test_iq)

#Converted year&week to float so I could cutoff easier.  Then deleted once sorted.
#   
#I'll use 1990 - 2005 as my training set and 2006 - 2010 as the validation set
train_cutoff = 2006

X_train_sj_ = X_train_sj.loc[X_train_sj.year < train_cutoff]
X_train_iq_ = X_train_iq.loc[X_train_iq.year < train_cutoff]
y_train_sj_ = y_train_sj.loc[y_train_sj.year < train_cutoff]
y_train_iq_ = y_train_iq.loc[y_train_iq.year < train_cutoff]


X_val_sj_ = X_train_sj.loc[X_train_sj.year >= train_cutoff]
X_val_iq_ = X_train_iq.loc[X_train_iq.year >= train_cutoff]
y_val_sj_ = y_train_sj.loc[y_train_sj.year >= train_cutoff]
y_val_iq_ = y_train_iq.loc[y_train_iq.year >= train_cutoff]


y_train_sj_lite = y_train_sj_['total_cases']
y_train_iq_lite = y_train_iq_['total_cases']

y_val_sj_lite = y_val_sj_['total_cases']
y_val_iq_lite = y_val_iq_['total_cases']

display(X_val_sj_.shape,y_val_sj_.shape,X_train_sj_.shape,y_train_sj_.shape)



X_train_sj_.drop(columns=['week_start_date'],inplace=True)
X_train_iq_.drop(columns=['week_start_date'],inplace=True)

X_val_iq_.drop(columns=['week_start_date'],inplace=True)
X_val_sj_.drop(columns=['week_start_date'],inplace=True)

test_sj.drop(columns=['week_start_date'],inplace=True)
test_iq.drop(columns=['week_start_date'],inplace=True)

from sklearn.pipeline import make_pipeline
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, mean_absolute_error
from sklearn.model_selection import GridSearchCV
import xgboost as xgb
from xgboost import XGBRegressor

import matplotlib.pyplot as plt

model_rf_sj = make_pipeline(
    SimpleImputer(),
    #OrdinalEncoder(),
    XGBRegressor(n_estimators=300,random_state=42)
)
model_rf_sj.fit(X_train_sj_,y_train_sj_lite)
print('SJ Training MAE:',mean_absolute_error(y_train_sj_lite,model_rf_sj.predict(X_train_sj_)))
print('SJ Training R^2:',model_rf_sj.score(X_train_sj_,y_train_sj_lite))

#Dual grid test
params = {
    'xgbregressor__n_estimators': [50,60,70,100,150,200,300],
    #'xgbregressor__max_features': ['auto','sqrt'],
    'xgbregressor__max_depth': [2,3,4,5,6,7,8,9,10,20,30,40,50]
}

model_rf_sj_gd = GridSearchCV(model_rf_sj,
                              param_grid=params,
                              n_jobs=-1,
                              #cv=5,
                              verbose=10)
model_rf_sj_gd.fit(X_train_iq_,y_train_iq_lite)
model_rf_sj_gd.best_estimator_

Xgbcoefs = model_rf_sj.named_steps.xgbregressor.feature_importances_
Xgbcoefs
test = pd.DataFrame(Xgbcoefs,X_train_sj_.columns)
test.sort_values(by=0,ascending=False)

model_rf_iq = make_pipeline(
    SimpleImputer(),
    #OrdinalEncoder(),
    XGBRegressor(n_estimators=300,random_state=42)
)
model_rf_iq.fit(X_train_iq_,y_train_iq_lite)
print('IQ Training MAE:',mean_absolute_error(y_train_iq_lite,model_rf_iq.predict(X_train_iq_)))
print('IQ Training R^2:',model_rf_iq.score(X_train_iq_,y_train_iq_lite))

#model_rf_iq.named_steps['randomforestregressor'].

#Dual grid test
params = {
    'xgbregressor__n_estimators': [50,60,70,100,150,200,300],
    #'xgbregressor__max_features': ['auto','sqrt'],
    'xgbregressor__max_depth': [1,2,3,4,5,6,7,8,9,10,20,30,40,50]
}

model_rf_iq_gd = GridSearchCV(model_rf_iq,
                              param_grid=params,
                              n_jobs=-1,
                              #cv=5,
                              verbose=10)
model_rf_iq_gd.fit(X_train_iq_,y_train_iq_lite)
model_rf_iq_gd.best_estimator_

#check against validation set
sjp = model_rf_sj_gd.best_estimator_
val_sj_score = mean_absolute_error(y_val_sj_lite, sjp.predict(X_val_sj_))
siq = model_rf_iq_gd.best_estimator_
val_iq_score = mean_absolute_error(y_val_iq_lite, siq.predict(X_val_iq_))
print(val_sj_score, val_iq_score)

sumb_b = model_rf_sj_gd.best_estimator_.predict(test_sj)
sumb_a = model_rf_iq_gd.best_estimator_.predict(test_iq)

dates_sj = test_sj[['year','weekofyear']]
dates_sj.insert(0,'city','sj')
dates_sj['total_cases'] = sumb_b.astype('int')

dates_iq = test_iq[['year','weekofyear']]
dates_iq.insert(0,'city','iq')
dates_iq['total_cases'] = sumb_a.astype('int')

DengWitt = pd.concat([dates_sj,dates_iq])
DengWitt

from google.colab import files
DengWitt.index=DengWitt['city']
DengWitt.drop(columns='city',inplace=True)
DengWitt.to_csv('DengWittOne.csv')
files.download('DengWittOne.csv')

submission